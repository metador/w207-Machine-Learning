{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invasive Species with TensorFlow and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load important modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the necessary modules to start working on SVMs\n",
    "from PIL import Image\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import random_seed\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "\n",
    "def reset_graph(seed=1337):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()\n",
    "DEFAULT_PATH = \"D:/MIDS/W207_3 Machine Learning/project_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for managing data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def load_images(indices, path = DEFAULT_PATH):\n",
    "#     images= np.zeros(shape=(len(indices), 866, 1154, 3), dtype=np.uint8)\n",
    "#     for p, i in enumerate(indices):\n",
    "#         im = Image.open(path+str(i+1)+\".jpg\")\n",
    "#         images[p] = np.array(im)\n",
    "#     images = images.astype(np.float32)\n",
    "#     images = np.multiply(images, 1.0 / 255.0)\n",
    "#     return images\n",
    "\n",
    "def load_images_flat(indices, labels, path = DEFAULT_PATH, seq = None):\n",
    "    # resize to (400,300 from now and flatten array)\n",
    "    # images= np.zeros(shape=(len(indices), 1440000), dtype=np.uint8)\n",
    "    images= np.zeros(shape=(len(indices), 2998092), dtype=np.uint8)\n",
    "    new_labels = np.zeros(shape = (labels.shape[0],))\n",
    "    if seq:\n",
    "        images= np.zeros(shape=(len(indices)*2, 2998092), dtype=np.uint8)\n",
    "        new_labels = np.zeros(shape = (labels.shape[0]*2,))\n",
    "    index = 0\n",
    "    for p, i in enumerate(indices):\n",
    "        im = Image.open(path+str(i+1)+\".jpg\")\n",
    "        # doing an extra resize\n",
    "        # arr = np.array(im.resize((800,600))).reshape(1,1440000)\n",
    "        vanilla_arr = np.array(im).reshape(1,2998092)\n",
    "        images[index] = vanilla_arr\n",
    "        new_labels[index]=labels[p]\n",
    "        if seq:\n",
    "            aug_arr = seq.augment_images([np.array(im)])[0].reshape(1,2998092)\n",
    "            index +=1\n",
    "            images[index] = aug_arr\n",
    "            new_labels[index]=labels[p]\n",
    "        index +=1\n",
    "    images = images.astype(np.float32)\n",
    "    images = np.multiply(images, 1.0 / 255.0)\n",
    "    return images, new_labels\n",
    "\n",
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                image_indices,\n",
    "                labels,\n",
    "                dataPath = DEFAULT_PATH,\n",
    "                seed=None,\n",
    "                aug_array = []):\n",
    "        seed1, seed2 = random_seed.get_seed(seed)\n",
    "        # If op level seed is not set, use whatever graph level seed is returned\n",
    "        np.random.seed(seed1 if seed is None else seed2)\n",
    "        self._num_examples = len(image_indices)\n",
    "        self._image_indices = image_indices\n",
    "        self._labels = labels\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        self.path = dataPath\n",
    "        self.aug_array = aug_array\n",
    "\n",
    "    @property\n",
    "    def image_indices(self):\n",
    "        return self._image_indices\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size, shuffle=True):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        \n",
    "        # pick augmentation sequence to apply.\n",
    "        # applying an augmentation sequence means we will return twice the amount of batch_size items,\n",
    "        # as each image sample will be read, and a copy of it will be augmented, making the samples 2*batch_size\n",
    "        aug_seq = np.random.choice(self.aug_array, 1)[0] if self.aug_array else None\n",
    "        \n",
    "        start = self._index_in_epoch\n",
    "        # Shuffle for the first epoch\n",
    "        if self._epochs_completed == 0 and start == 0 and shuffle:\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._image_indices = self.image_indices[perm0]\n",
    "            self._labels = self.labels[perm0]\n",
    "        # Go to the next epoch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = self._num_examples - start\n",
    "            images_rest_part = self._image_indices[start:self._num_examples]\n",
    "            labels_rest_part = self._image_indices[start:self._num_examples]\n",
    "            # Shuffle the data\n",
    "            if shuffle:\n",
    "                perm = np.arange(self._num_examples)\n",
    "                np.random.shuffle(perm)\n",
    "                self._image_indices = self.image_indices[perm]\n",
    "                self._labels = self.labels[perm]\n",
    "          # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self._index_in_epoch\n",
    "            images_new_part = self._image_indices[start:end]\n",
    "            labels_new_part = self._labels[start:end]\n",
    "            labels = np.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "            images, labels= load_images_flat(np.concatenate((images_rest_part, images_new_part), axis =0), labels, path = self.path, seq = aug_seq)\n",
    "            return images, labels\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            labels = self._labels[start:end]\n",
    "            images, labels = load_images_flat(self._image_indices[start:end], labels, path = self.path, seq = aug_seq)            \n",
    "            return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Sequences of Image Augmentation and specify which one to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see https://github.com/aleju/imgaug\n",
    "flip_lr_seq = iaa.Sequential([\n",
    "    iaa.Fliplr(1), # horizontally flip\n",
    "])\n",
    "\n",
    "flip_ud_seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), # vertically flip\n",
    "])\n",
    "\n",
    "gauss_1_seq = iaa.Sequential([\n",
    "    iaa.GaussianBlur(1.0), # blur images with a sigma of 0 to 3.0\n",
    "])\n",
    "\n",
    "flip_seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), # vertically flip\n",
    "    iaa.Fliplr(1), # horizontally flip\n",
    "])\n",
    "\n",
    "flip_ud_gauss_seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), # vertically flip\n",
    "    iaa.GaussianBlur(1.0),\n",
    "])\n",
    "\n",
    "flip_lr_gauss_seq = iaa.Sequential([\n",
    "    iaa.Fliplr(1), # vertically flip\n",
    "    iaa.GaussianBlur(1.0),\n",
    "])\n",
    "\n",
    "flip_2_gauss_seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), # vertically flip\n",
    "    iaa.Fliplr(1), # horizontally flip\n",
    "    iaa.GaussianBlur(1.0),\n",
    "])\n",
    "# pass this array to DataSet object. during next_batch() one of the seq in the array will be picked randomly and applied when loading data\n",
    "seq_array = [flip_lr_seq, flip_ud_seq, gauss_1_seq, flip_seq, flip_ud_gauss_seq, flip_lr_gauss_seq, flip_2_gauss_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Sequential(name=UnnamedSequential, augmenters=[Flipud(name=UnnamedFlipud, parameters=[Binomial(Deterministic(float 1.00000000))], deterministic=False), GaussianBlur(name=UnnamedGaussianBlur, parameters=[Deterministic(float 1.00000000)], deterministic=False)], deterministic=False)]\n",
      "[[ 32  73  33]\n",
      " [ 42  81  52]\n",
      " [ 61  87  76]\n",
      " ..., \n",
      " [ 90 136 123]\n",
      " [101 154 136]\n",
      " [111 174 143]]\n",
      "[[ 58  82  82]\n",
      " [ 62  80  81]\n",
      " [ 66  78  78]\n",
      " ..., \n",
      " [127 157  86]\n",
      " [130 167 103]\n",
      " [108 158  80]]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "print(np.random.choice(seq_array, 1))\n",
    "\n",
    "im = Image.open(\"D:/MIDS/W207_3 Machine Learning/project_data/train/\"+str(1)+\".jpg\")\n",
    "seq = iaa.Sequential([\n",
    "#     iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "    iaa.Fliplr(1), # horizontally flip 50% of the images\n",
    "#     iaa.GaussianBlur(sigma=(0, 3.0)) # blur images with a sigma of 0 to 3.0\n",
    "])\n",
    "arr = np.array(im)\n",
    "print(arr[:,0,:])\n",
    "im_aug = seq.augment_images(arr)\n",
    "print(im_aug[:,-1,:])\n",
    "# arr = np.array(im.resize((1200,900))).reshape(1,1440000)\n",
    "# arr = np.array(im.resize((1,2998092)))\n",
    "# arr = np.array(im).reshape(1,2998092)\n",
    "print(np.array_equal(arr,im_aug))\n",
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train/test data and labels for working with tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare labels\n",
    "train_labels = read_csv(DEFAULT_PATH + \"train_labels.csv\")['invasive'].values\n",
    "y_train = train_labels[:1800]\n",
    "# y_dev = train_labels[1800:]\n",
    "y_validation = train_labels[1800:]\n",
    "\n",
    "# prepare data\n",
    "train_data = DataSet(np.arange(0,1800), y_train, dataPath = DEFAULT_PATH + \"train/\", aug_array = seq_array)\n",
    "# test_data = DataSet(np.arange(1800,len(train_labels)), y_dev, dataPath = DEFAULT_PATH +\"test/\")\n",
    "validation_data = DataSet(np.arange(1800,len(train_labels)), y_validation, dataPath = DEFAULT_PATH + \"train/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4590\n",
      "(4590,)\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "print(train_labels.shape[0]*2)\n",
    "print(np.zeros(shape = (train_labels.shape[0]*2,)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Preparation: \n",
    "\n",
    "From my notes on choosing Kernel: Linear SVM (no kernel): Many training features, few training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svmC = 0.001\n",
    "num_features = 2998092 #CHANGE WHEN RESIZING\n",
    "n_epochs = 30\n",
    "batch_size = 80\n",
    "best_loss_val = np.infty\n",
    "check_interval = 5\n",
    "checks_since_last_progress = 0\n",
    "max_checks_without_progress = 8\n",
    "best_model_params = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set graph\n",
    "reset_graph()\n",
    "\n",
    "# inputs\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_features], name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=[None], name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "# set weights and create function\n",
    "W = tf.Variable(tf.zeros([num_features,1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "# y_raw = tf.matmul(X,W) + b\n",
    "y_raw = tf.matmul(X,W) - b\n",
    "\n",
    "# Optimization.\n",
    "# regularization_loss = 0.5*tf.reduce_sum(tf.square(W))\n",
    "regularization_loss = tf.reduce_sum(tf.square(W))\n",
    "# hinge_loss = tf.reduce_sum(tf.maximum(tf.zeros([batch_size,1]), 1 - y*y_raw))\n",
    "\n",
    "# HACK: *2 needed as the training involves augmentation, which adds each image again in an augmented form, so we get twice as big an array from load_image_flat\n",
    "hinge_loss = tf.reduce_mean(tf.maximum(tf.zeros([batch_size*2,1]), 1 - y*y_raw))\n",
    "\n",
    "# svm_loss = regularization_loss + svmC*hinge_loss\n",
    "svm_loss = svmC*regularization_loss + hinge_loss\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(svm_loss)\n",
    "\n",
    "# HACK: SVM Loss for validation set; TODO find a better way to do this\n",
    "# This is needed because the training involves augmentation, which adds each image again in an augmented form\n",
    "# This doesn't happen for validation\n",
    "hinge_loss_test = tf.reduce_mean(tf.maximum(tf.zeros([batch_size,1]), 1 - y*y_raw))\n",
    "svm_loss_test = svmC*regularization_loss + hinge_loss_test\n",
    "\n",
    "# Evaluation.\n",
    "predicted_class = tf.sign(y_raw);\n",
    "correct_prediction = tf.equal(y,predicted_class)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define session and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 2998092)\n",
      "(160,)\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "X_val, y_val = train_data.next_batch(80)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "# with tf.Session() as sess:\n",
    "#     init.run()\n",
    "#     val = y_raw.eval(feed_dict={X: X_val, y: y_val})\n",
    "#     print(\"accuracy\")\n",
    "#     acc_val = accuracy.eval(feed_dict={X: X_val, y: y_val})\n",
    "#     print(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "Epoch 0, train accuracy: 66.2500%, valid. accuracy: 65.0000%, valid. best loss: 0.366802\n",
      "epoch = 1\n",
      "Epoch 1, train accuracy: 61.2500%, valid. accuracy: 65.0000%, valid. best loss: 0.366787\n",
      "epoch = 2\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = validation_data.next_batch(80)\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        # start an epock\n",
    "        print(\"epoch = \"+str(epoch))\n",
    "        # print(\"W = \"+str(W.eval()))\n",
    "        for iteration in range(train_data.num_examples // batch_size):\n",
    "            # get a batch to run\n",
    "            X_batch, y_batch = train_data.next_batch(batch_size)\n",
    "            # run the train\n",
    "            # print(X_batch.shape)\n",
    "            sess.run(train_step, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "            loss_val = svm_loss_test.eval(feed_dict={X: X_val,y: y_val})\n",
    "            if loss_val < best_loss_val:\n",
    "                best_loss_val = loss_val\n",
    "                checks_since_last_progress = 0\n",
    "            else:\n",
    "                checks_since_last_progress += 1\n",
    "        # check accuracy\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_val, y: y_val})\n",
    "        print(\"Epoch {}, train accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(epoch, acc_train * 100, acc_val * 100, best_loss_val))\n",
    "        if checks_since_last_progress > max_checks_without_progress:\n",
    "            print(\"Early stopping!\")\n",
    "            print()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
